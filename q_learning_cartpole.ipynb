{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q(state, action, q_weights):\n",
    "    return np.dot(np.append(state, action), q_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(env, obs, q_weights, epsilon):\n",
    "    action = None\n",
    "    e = random.random()\n",
    "    if e < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        possible_actions = range(env.action_space.n)\n",
    "        q_values = {Q(obs, a, q_weights) : a for a in possible_actions}\n",
    "        action = q_values[max(q_values)]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_target(env, transition, q_weights, discount_rate):\n",
    "    _, _, t_reward, t_new_obs, t_terminal = transition\n",
    "    if t_terminal:\n",
    "        target = t_reward\n",
    "    else:\n",
    "        possible_actions = range(env.action_space.n)\n",
    "        max_Q = max([Q(t_new_obs, a, q_weights) for a in possible_actions])\n",
    "        target = transition[2] + discount_rate * max_Q\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def gradient_descent(target, q_weights, obs, action, learning_rate):\n",
    "    return q_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODES = 5\n",
    "MAX_TIMESTEPS = 50\n",
    "MINIBATCH_SIZE = 5\n",
    "EPSILON = .1\n",
    "DISCOUNT_RATE = .9\n",
    "LEARNING_RATE = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/.pyenv/versions/3.8.18/envs/drl/lib/python3.8/site-packages/gymnasium/envs/classic_control/cartpole.py:180: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "replay_memory = []\n",
    "q_weights = rng.random(env.observation_space.shape[0] + 1)\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    states = [obs]\n",
    "    for t in range(MAX_TIMESTEPS):\n",
    "        # sample and take action\n",
    "        action = epsilon_greedy(env, obs, q_weights, EPSILON)\n",
    "        new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        # sample minibatch and update q_weights\n",
    "        replay_memory.append([obs, action, reward, new_obs, terminated or truncated])\n",
    "        minibatch = random.sample(replay_memory, min(MINIBATCH_SIZE, len(replay_memory)))\n",
    "        for transition in minibatch:\n",
    "            target = calculate_target(env, transition, q_weights, DISCOUNT_RATE)\n",
    "            t_obs, t_action, _, _, _ = transition\n",
    "            q_weights = gradient_descent(target, q_weights, t_obs, t_action, LEARNING_RATE)\n",
    "        obs = new_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "obs, _ = env.reset()\n",
    "for _ in range(100):\n",
    "    action = epsilon_greedy(env, obs, q_weights, EPSILON)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
